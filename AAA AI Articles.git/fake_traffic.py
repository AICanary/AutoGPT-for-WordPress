# from fake_traffic import fake_traffic
# 
# fake_traffic(country='US', language='en-US')

# from fake_traffic import fake_traffic
# 
# fake_traffic(country="TR", language="ku-TR", category='h', threads=2)


from random import uniform, choice, randint, sample, shuffle
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from time import sleep
from itertools import zip_longest

import requests
from lxml import html
from duckduckgo_search import ddg
from google_searching import ggl
from google_trends import daily_trends, realtime_trends

__version__ = '1.8.2'

THREADS = 2
MIN_WAIT = 1
MAX_WAIT = 60
DEBUG = False

BLACKLIST = (
    "bit.ly", "clickserve", "https://t.co", "itunes.apple.com", "javascript:",
    "l.facebook.com", "legal.twitter.com", "login", "Login", "mail.", "mailto:", 
    "mediawiki", "messenger.com", "policies", "s.click", "showcaptcha?", "signup",
    "smart-captcha/", "Special:", "support.", "t.umblr.com", "tel:", "tg://",
    "whatsapp://", "zendesk", "_click_",
    "/auth/", "/authorize?", "/captcha", "/chat", "/click", "/feed?", "/join?",
    "/joinchat", "/help", "/privacy", "/registration", "/share", "/showcaptcha",
    "/stat/", "/support", "/terms", "/tos", "/tweet",
    ".cs", ".css", ".gif", ".iso", ".jpg", ".jpeg", ".ico", ".js", ".json", ".png",
    ".svg", ".xml",
    )
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; rv:102.0) Gecko/20100101 Firefox/102.0",
    }
URLS_CACHE = set()



def grouper(iterable, n):
    "Collect data into non-overlapping fixed-length chunks or blocks"
    # grouper('ABCDEFG', 3) --> ABC DEF GNoneNone
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=None)

def debug_print(*agrs, **kwargs):
    if DEBUG:
        print(*agrs, **kwargs)

def get_yesterday():
    yesterday = datetime.now() - timedelta(1)
    return datetime.strftime(yesterday, '%Y%m%d')

def real_trends(country='US', language='en-US', category='h'):        
    while True:
        try:
            try:
                trends = realtime_trends(country=country, language=language, category=category, num_results=20)
                return trends
            except:
                print(f'Google realtime trends error. Trying daily trends.')
                trends = daily_trends(country=country, language=language) + daily_trends(get_yesterday(), country=country, language=language)
                return trends
        except:
            print(f'Google trends error. Sleep 25-35 sec')
            sleep(uniform(25, 35))

def url_in_blacklist(url):
    if any(x in url for x in BLACKLIST):
        debug_print(f'{url}, STATUS: in BLACKLIST')
        return True

def url_fix(url):
    url = url.strip('.')
    if 'https://' not in url and 'http://' not in url:
        url = f"https://{url}"
    try:
      url = url[:url.rindex('#')]
    except:
      pass
    return url    

def get_url(url):
    url = url_fix(url)
    if url not in URLS_CACHE and not url_in_blacklist(url):
        debug_print(f'{url}, STATUS: request')
        try:            
            resp = requests.get(url, headers=HEADERS, timeout=4)
            URLS_CACHE.add(url)
            if resp.status_code == 200:
                debug_print(f'{resp.url}, STATUS: {resp.status_code}')
                if url_in_blacklist(resp.url):
                    return None            
                return resp
            debug_print(resp.raise_for_status())      
        except requests.ConnectionError:
            debug_print(f'{url}, STATUS: Connection error. Sleep 25-35 sec')
            sleep(uniform(25, 35))
        except:
            debug_print(f'{url}, STATUS: ERROR')

def google_search(word, max_results=20):
    query  = word.replace(' ','+')
    search_ggl = ggl(query, max_results=max_results)
    search_ggl = search_ggl[:max_results]
    urls = [x['href'].lstrip('https://') for x in search_ggl]
    return urls

def ddg_search(word, max_results=20):
    query  = word.replace(' ','+')
    search_ddg = ddg(query, max_results=max_results)
    search_ddg = search_ddg[:max_results]
    urls = [x['href'].lstrip('https://') for x in search_ddg]
    return urls

def parse_urls(response):
    try:
        tree = html.fromstring(response.text)
        tree.make_links_absolute(response.url)
        urls = tree.xpath('//a/@href')
        urls = [url for url in urls if not any(x in url for x in BLACKLIST)]
        return urls
    except:
        return []

def recursive_browse(url, depth=randint(0, 5)):
    if not depth:
        get_url(url)
        return
    resp = get_url(url)
    if resp:
        recursive_urls = parse_urls(resp)
        if recursive_urls:
            url = choice(recursive_urls)
            sleep(uniform(MIN_WAIT, MAX_WAIT))
            recursive_browse(url, depth-1)

def _thread(trend):
    if isinstance(trend, dict):
        word = ' '.join(sample(trend['entity_names'], 2))
        article_urls = trend['article_urls']
    else:
        word = trend
        article_urls = []
    print(f"*** Trend = {word} ***")
    
    google_urls, ddg_urls = [], []
    try:
        google_urls = google_search(word)
        print(f"'{word}': google_urls len = {len(google_urls)}")
        article_urls.extend(url for url in google_urls if url not in article_urls)
    except:
        print('google search error')
    try:
        ddg_urls = ddg_search(word)
        print(f"'{word}': ddg_urls len = {len(ddg_urls)}")
        article_urls.extend(url for url in ddg_urls if url not in article_urls)
    except:
        print('ddg search error')
        
    print(f"Recursive browsing {len(article_urls)} urls. Wait...", end='\n\n')    
    for i, url in enumerate(article_urls, start=1):
        debug_print(f"{i}/{len(article_urls)} recursive browse")
        if i <= 10:
            min_depth, max_depth = 15, 25
        else:
            min_depth, max_depth = 0, 5
        recursive_browse(url, depth=randint(min_depth, max_depth))    
    
def fake_traffic(country='US', language='en-US', category='h', threads=THREADS, min_wait=MIN_WAIT, max_wait=MAX_WAIT, debug=DEBUG):
    """ Imitating an Internet user by mimicking popular web traffic (internet traffic generator).    
    country = country code ISO 3166-1 Alpha-2 code (https://www.iso.org/obp/ui/),
    language = country-language code ISO-639 and ISO-3166 (https://www.fincher.org/Utilities/CountryLanguageList.shtml),
    category = сategory of interest of a user (defaults to 'h'):
               'all' (all), 'b' (business), 'e' (entertainment), 
               'm' (health), 's' (sports), 't' (sci/tech), 'h' (top stories);
    threads = number of threads (defaults to 1),
    min_wait = minimal delay between requests (defaults to 1),
    max_wait = maximum delay between requests (defaults to 30),
    debug = if True, then print the details of the requests (defaults to False).
    """
  
    global THREADS, MIN_WAIT, MAX_WAIT, DEBUG
    THREADS = threads
    MIN_WAIT = min_wait
    MAX_WAIT = max_wait
    DEBUG = debug
    
    print(f'*** Fake traffic {__version__} ***')
    if category not in ('all', 'b', 'e', 'm', 's', 't', 'h'):
        print('''Wrong category, specify the correct category:\n'all' (all), 'b' (business), 'e' (entertainment),\n'm' (health), 's' (sports), 't' (sci/tech), 'h' (top stories);''')
        return
    while True:
        print(f'\n{datetime.now()}')
        print(f'---GET TRENDS IN {country=} {language=} {category=}---')
        trends = real_trends(country=country, language=language, category=category)
        #trends = sample(trends, threads)
        shuffle(trends)
        for temp_trends in grouper(trends, threads):
            with ThreadPoolExecutor(threads) as executor:
                for i, trend in enumerate(temp_trends, start=1):
                    if trend:
                        print(f"Thread {i} start.")
                        executor.submit(_thread, trend)
                        sleep(uniform(25, 35))
        URLS_CACHE.clear()

if __name__ == '__main__':
    fake_traffic(country='US', language='en-US')
